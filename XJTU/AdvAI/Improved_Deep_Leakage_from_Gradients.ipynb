{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NiceAsiv/Homework/blob/main/XJTU/AdvAI/Improved_Deep_Leakage_from_Gradients.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yNFySBucRyv"
      },
      "source": [
        "IDLG（Improved Deep Leakage from Gradients）是在DLG方法的基础上进行改进。我们发现共享梯度肯定会泄露ground-truth的标签。我们提出了一种简单而可靠的方法来从梯度中提取准确的数据。特别是，相对于DLG来说，我们的方法肯定能提取出真实的标签，因此我们将其命名为改进的DLG（iDLG）。\n",
        "项目地址：https://github.com/PatrickZH/Improved-Deep-Leakage-from-Gradients/tree/master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gao2W802cRyw"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import pickle\n",
        "import PIL.Image as Image\n",
        "\n",
        "\n",
        "torch.manual_seed(50)\n",
        "print(torch.__version__, torchvision.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmtQUpcJcRyw"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = 'cuda' if use_cuda else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvLsnhtlcRyw"
      },
      "outputs": [],
      "source": [
        "class LeNet(nn.Module):\n",
        "    def __init__(self, channel=3, hideen=768, num_classes=10):\n",
        "        super(LeNet, self).__init__()\n",
        "        act = nn.Sigmoid\n",
        "        self.body = nn.Sequential(\n",
        "            nn.Conv2d(channel, 12, kernel_size=5, padding=5 // 2, stride=2),\n",
        "            act(),\n",
        "            nn.Conv2d(12, 12, kernel_size=5, padding=5 // 2, stride=2),\n",
        "            act(),\n",
        "            nn.Conv2d(12, 12, kernel_size=5, padding=5 // 2, stride=1),\n",
        "            act(),\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hideen, num_classes)\n",
        "        )\n",
        "\n",
        "    # forward method\n",
        "    def forward(self, x):\n",
        "        out = self.body(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "#===============初始化权重=================\n",
        "def weights_init(m):\n",
        "    try:\n",
        "        # hasattr()函数 用于判断对象是否包含对应的属性。\n",
        "        if hasattr(m, \"weight\"):\n",
        "            # 使用均匀分布U(a,b)初始化Tensor，即Tensor的填充值是等概率的范围为 [a，b) 的值。均值为 （a + b）/ 2.\n",
        "            m.weight.data.uniform_(-0.5, 0.5)\n",
        "    except Exception:\n",
        "        print('warning: failed in weights_init for %s.weight' % m._get_name())\n",
        "    try:\n",
        "        if hasattr(m, \"bias\"):\n",
        "            m.bias.data.uniform_(-0.5, 0.5)\n",
        "    except Exception:\n",
        "        print('warning: failed in weights_init for %s.bias' % m._get_name())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWUANsUvcRyx",
        "outputId": "7ecbf118-1907-4623-becc-d518533dd763"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
          ]
        }
      ],
      "source": [
        "def load_data(dataset, root_path):\n",
        "    data_path = os.path.join(root_path, './torch/%s'%dataset).replace('\\\\', '/')\n",
        "    if dataset == 'MNIST':\n",
        "        shape_img = (28, 28)\n",
        "        num_classes = 10\n",
        "        channel = 1\n",
        "        hidden = 588\n",
        "        dst = datasets.MNIST(data_path, download=True)\n",
        "    elif dataset == 'cifar100':\n",
        "        shape_img = (32, 32)\n",
        "        num_classes = 100\n",
        "        channel = 3\n",
        "        hidden = 768\n",
        "        dst = datasets.CIFAR100(data_path, download=True)\n",
        "    else:\n",
        "        exit('unknown dataset')\n",
        "    return dst, shape_img, num_classes, channel, hidden\n",
        "\n",
        "\n",
        "def train_iDLG(net, gt_data, gt_label, original_dy_dx, device, lr=1.0, Iteration=300):\n",
        "\n",
        "    # Initialize dummy data\n",
        "    dummy_data = torch.randn(gt_data.size()).to(device).requires_grad_(True)\n",
        "    label_pred = torch.argmin(torch.sum(original_dy_dx[-2], dim=-1), dim=-1).detach().reshape((1,))\n",
        "    optimizer = torch.optim.LBFGS([dummy_data], lr=lr)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    # Training loop\n",
        "    losses, mses = [], []\n",
        "    for iters in range(Iteration):\n",
        "        def closure():\n",
        "            optimizer.zero_grad()\n",
        "            pred = net(dummy_data)\n",
        "            dummy_loss = criterion(pred, label_pred)\n",
        "            dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True)\n",
        "\n",
        "            grad_diff = 0\n",
        "            for gx, gy in zip(dummy_dy_dx, original_dy_dx):\n",
        "                grad_diff += ((gx - gy) ** 2).sum()\n",
        "            grad_diff.backward()\n",
        "            return grad_diff\n",
        "\n",
        "        optimizer.step(closure)\n",
        "        current_loss = closure().item()\n",
        "        losses.append(current_loss)\n",
        "        mses.append(torch.mean((dummy_data - gt_data) ** 2).item())\n",
        "\n",
        "        if current_loss < 1e-6:\n",
        "            break\n",
        "\n",
        "    return losses, mses, dummy_data\n",
        "\n",
        "def train_DLG(net, gt_data, gt_label, original_dy_dx, num_classes, device, lr=1.0, Iteration=300):\n",
        "    # Initialize dummy data and label\n",
        "    dummy_data = torch.randn(gt_data.size()).to(device).requires_grad_(True)\n",
        "    dummy_label = torch.randn((gt_data.shape[0], num_classes)).to(device).requires_grad_(True)\n",
        "    optimizer = torch.optim.LBFGS([dummy_data, dummy_label], lr=lr)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    # Training loop\n",
        "    losses, mses = [], []\n",
        "    for iters in range(Iteration):\n",
        "        def closure():\n",
        "            optimizer.zero_grad()\n",
        "            pred = net(dummy_data)\n",
        "            dummy_loss = - torch.mean(torch.sum(torch.softmax(dummy_label, -1) * torch.log(torch.softmax(pred, -1)), dim=-1))\n",
        "            dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True)\n",
        "\n",
        "            grad_diff = 0\n",
        "            for gx, gy in zip(dummy_dy_dx, original_dy_dx):\n",
        "                grad_diff += ((gx - gy) ** 2).sum()\n",
        "            grad_diff.backward()\n",
        "            return grad_diff\n",
        "\n",
        "        optimizer.step(closure)\n",
        "        current_loss = closure().item()\n",
        "        losses.append(current_loss)\n",
        "        mses.append(torch.mean((dummy_data - gt_data) ** 2).item())\n",
        "\n",
        "        if current_loss < 1e-6:\n",
        "            break\n",
        "\n",
        "    return losses, mses, dummy_data\n",
        "\n",
        "def compute_original_gradient(net, gt_data, gt_label, criterion):\n",
        "    out = net(gt_data)\n",
        "    y = criterion(out, gt_label)\n",
        "\n",
        "    # 计算y关于参数的梯度\n",
        "    dy_dx = torch.autograd.grad(y, net.parameters())\n",
        "    # 将所有参数的梯度克隆存储为列表\n",
        "    original_dy_dx = [param_grad.detach().clone() for param_grad in dy_dx]\n",
        "    return original_dy_dx\n",
        "\n",
        "def plot_results(losses, mses, dummy_data, gt_data, dataset):\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(losses)\n",
        "    plt.title('Losses')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(mses)\n",
        "    plt.title('MSE')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('MSE')\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(np.transpose(dummy_data.detach().cpu().numpy().reshape(gt_data.shape), (0, 2, 3, 1))[0])\n",
        "    plt.title('Dummy Data')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xnbSkBzcRyx"
      },
      "outputs": [],
      "source": [
        "dataset = 'MNIST'\n",
        "root_path = '.'\n",
        "data_path = os.path.join(root_path, './torch/MNIST').replace('\\\\', '/')\n",
        "save_path = os.path.join(root_path, './results/iDLG_%s'%dataset).replace('\\\\', '/')\n",
        "if not os.path.exists(save_path):\n",
        "    os.mkdir(save_path)\n",
        "if not os.path.exists('results'):\n",
        "    os.mkdir('results')\n",
        "\n",
        "print(dataset, 'root_path:', root_path)\n",
        "print(dataset, 'data_path:', data_path)\n",
        "print(dataset, 'save_path:', save_path)\n",
        "\n",
        "#load data\n",
        "dst, shape_img, num_classes, channel, hidden = load_data(dataset, root_path)\n",
        "\n",
        "tt = transforms.Compose([transforms.ToTensor()])\n",
        "tp = transforms.Compose([transforms.ToPILImage()])\n",
        "\n",
        "\n",
        "lr = 1.0\n",
        "num_dummy = 1\n",
        "Iteration = 300\n",
        "num_exp = 10\n",
        "\n",
        "\n",
        "for idx_net in range(num_exp):\n",
        "    net = LeNet(channel=channel, hideen=hidden, num_classes=num_classes)\n",
        "    net.apply(weights_init)\n",
        "\n",
        "    print('running %d|%d experiment'%(idx_net, num_exp))\n",
        "    net = net.to(device)\n",
        "    idx_shuffle = np.random.permutation(len(dst))\n",
        "\n",
        "    for method in ['DLG', 'iDLG']:\n",
        "        criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "        gt_data, gt_label = None, None\n",
        "        imidx_list = []\n",
        "        for imidx in range(num_dummy):\n",
        "            idx = idx_shuffle[imidx]\n",
        "            imidx_list.append(idx)\n",
        "            tmp_datum = tt(dst[idx][0]).float().to(device)\n",
        "            tmp_datum = tmp_datum.view(1, *tmp_datum.size())\n",
        "            tmp_label = torch.Tensor([dst[idx][1]]).long().to(device)\n",
        "            tmp_label = tmp_label.view(1, )\n",
        "            if imidx == 0:\n",
        "                gt_data = tmp_datum\n",
        "                gt_label = tmp_label\n",
        "            else:\n",
        "                gt_data = torch.cat((gt_data, tmp_datum), dim=0)\n",
        "                gt_label = torch.cat((gt_label, tmp_label), dim=0)\n",
        "\n",
        "        original_dy_dx = compute_original_gradient(net, gt_data, gt_label, criterion)\n",
        "\n",
        "\n",
        "        if method == 'DLG':\n",
        "            losses, mses, dummy_data = train_DLG(net, gt_data, gt_label, original_dy_dx, num_classes, device, lr, Iteration)\n",
        "        elif method == 'iDLG':\n",
        "            losses, mses, dummy_data = train_iDLG(net, gt_data, gt_label, original_dy_dx, device, lr, Iteration)\n",
        "\n",
        "        plot_results(losses, mses, dummy_data, gt_data, dataset)\n",
        "        print(f'{method} Final Loss: {losses[-1]}, MSE: {mses[-1]}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}